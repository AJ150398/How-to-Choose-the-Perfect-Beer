# -*- coding: utf-8 -*-
"""cloud.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xvWze243THmWrNPP9DWMt6DbsAZD2Gpc
"""

# importing libraries
import numpy as np # for numerical computation
import matplotlib.pyplot as plt 
import pandas as pd # for importing and managing data sets
import seaborn as sns

# importing data set
from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

train_set = pd.read_csv('train.csv')
test_set = pd.read_csv('test.csv')

# check null values
train_null = train_set.isnull().sum()
test_null = test_set.isnull().sum()

""" train_null # Run Selection: Ctrl+Shift+Enter """

# onehotencode and combine
train_set_ohe = pd.DataFrame()
test_set_ohe = pd.DataFrame()

def my_ohe(X_train, X_test, column):
    global train_set_ohe
    global test_set_ohe
    
    train_set_ohe = pd.concat([train_set_ohe, pd.get_dummies(X_train[column], drop_first = True, prefix="_" + column)], axis = 1)
    test_set_ohe = pd.concat([test_set_ohe, pd.get_dummies(X_test[column], drop_first = True, prefix="_" + column)], axis = 1)

def my_combine(X_train, X_test, column):
    global train_set_ohe
    global test_set_ohe
    
    train_set_ohe = pd.concat([train_set_ohe, X_train[column]], axis = 1)
    test_set_ohe = pd.concat([test_set_ohe, X_test[column]], axis = 1)

# 1. summarize target variable
"""
train_set['Score'].describe() 

sns.distplot(train_set['Score'])
sum(train_set['Score'] == 0) # 26090

print("Skewness: %f" % train_set['Score'].skew()) # -1.711620
print("Kurtosis: %f" % train_set['Score'].kurt()) # 1.424491
"""

# 2. correlation matrix
"""
corrmat = train_set.corr()
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True)

# NO correlation found b/w any variable(s)
"""

# merge train and test sets
train_test = pd.DataFrame()
train_test = train_test.append(train_set)
train_test.drop('Score', axis = 1, inplace = True) # drop target variable
train_test = train_test.append(test_set)

# rows common in both train and test set
common_df = pd.merge(train_set, test_set, how='inner')
# sum(common_df['Score']==0)/(len(common_df['Score'])) # 1.0

# a) 'ABV'

train_set['ABV'].fillna(train_set['ABV'].median(), inplace = True)
test_set['ABV'].fillna(train_set['ABV'].median(), inplace = True) 

"""
FIP1:    No improvement
from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()
train_set['ABV_new'] = labelencoder.fit_transform(train_set['ABV'])

test_set.loc[test_set['ABV']==0.08, 'ABV'] = 0.06 
test_set.loc[test_set['ABV']==0.76, 'ABV'] = 0.8
test_set.loc[test_set['ABV']==1.7, 'ABV'] = 1.6
test_set.loc[test_set['ABV']==2.89, 'ABV'] = 2.9
test_set.loc[test_set['ABV']==3.17, 'ABV'] = 3.2
test_set.loc[test_set['ABV']==3.79, 'ABV'] = 3.8
test_set.loc[test_set['ABV']==4.27, 'ABV'] = 4.28
test_set.loc[test_set['ABV']==4.41, 'ABV'] = 4.39
test_set.loc[test_set['ABV']==4.54, 'ABV'] = 4.55
test_set.loc[test_set['ABV']==7.18, 'ABV'] = 7.19
test_set.loc[test_set['ABV']==7.27, 'ABV'] = 7.28
test_set.loc[test_set['ABV']==7.42, 'ABV'] = 7.44
test_set.loc[test_set['ABV']==7.68, 'ABV'] = 7.69
test_set.loc[test_set['ABV']==7.83, 'ABV'] = 7.82
test_set.loc[test_set['ABV']==8.06, 'ABV'] = 8.05
test_set.loc[test_set['ABV']==8.81, 'ABV'] = 8.8
test_set.loc[test_set['ABV']==8.94, 'ABV'] = 8.95
test_set.loc[test_set['ABV']==9.13, 'ABV'] = 9.12
test_set.loc[test_set['ABV']==9.43, 'ABV'] = 9.44
test_set.loc[test_set['ABV']==12.67, 'ABV'] = 12.66

test_set['ABV_new'] = labelencoder.transform(test_set['ABV'])
"""

"""
# FIP2: No improvement
train_set['ABV'].fillna(-1, inplace = True)
test_set['ABV'].fillna(-1, inplace = True) 
"""

"""
FIP3: No improvement
train_set['ABV'].fillna(train_set['ABV'].mean(), inplace = True)
test_set['ABV'].fillna(train_set['ABV'].mean(), inplace = True) 
"""

"""
FIP4: No improvement
train_set['ABV'].fillna(train_set['ABV'].mode()[0], inplace = True)
test_set['ABV'].fillna(train_set['ABV'].mode()[0], inplace = True) 
"""

"""
# FIP5: No improvement
ABV_dict = dict(train_set["Score"].groupby(train_set["ABV"]).mean())
train_set['ABV_new'] = train_set['ABV'].map(ABV_dict)
test_set['ABV_new'] = test_set['ABV'].map(ABV_dict)

test_set['ABV_new'].fillna(train_set['ABV_new'].median(), inplace = True)

my_combine(train_set, test_set, 'ABV_new')
"""

my_combine(train_set, test_set, 'ABV')

# b) 'Brewing Company' # a unique id given to each company [categorical variable (not numerical)]

my_combine(train_set, test_set, 'Brewing Company')

"""
# FIP1:

Brewing_Company_dict = dict(train_set["Score"].groupby(train_set["Brewing Company"]).mean())
train_set['new_Brewing_Company'] = train_set['Brewing Company'].map(Brewing_Company_dict)
test_set['new_Brewing_Company'] = test_set['Brewing Company'].map(Brewing_Company_dict)

test_set['new_Brewing_Company'].fillna(train_set['new_Brewing_Company'].mean(), inplace = True)


# each bin has same frequency (useful when data is skewed)
train_set['new_Brewing_Company'] = pd.qcut(train_set['new_Brewing_Company'], 8)
test_set['new_Brewing_Company'] = pd.qcut(test_set['new_Brewing_Company'], 8)

# OR

# each bin has equal width 
train_set['new_Brewing_Company_binned'] = pd.cut(train_set['new_Brewing_Company'].astype(int), 8)
test_set['new_Brewing_Company_binned'] = pd.cut(train_set['new_Brewing_Company'].astype(int), 8)

from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()
train_set['new_Brewing_Company_binned_enc'] = labelencoder.fit_transform(train_set['new_Brewing_Company_binned'])
test_set['new_Brewing_Company_binned_enc'] = labelencoder.transform(test_set['new_Brewing_Company_binned'])

my_combine(train_set, test_set, 'new_Brewing_Company_binned_enc')
"""

# c) 'Food Paring'

my_combine(train_set, test_set, 'Food Paring')

"""
# FIP1:

import re
import nltk

from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
nltk.download('stopwords')

food_paring_corpus_train = []

for i in range(len(train_set)):
    review = re.sub('[^a-zA-Z]', ' ', train_set['Food Paring'][i]) 
    review = review.lower() 
    review = review.split() 
    
    ps = PorterStemmer() 
    stop_words = set(stopwords.words('english'))
    # update stop_words
    stop_words.update(['.', ',', '"', "'", ':', ';', '(', ')', '[', ']', '{', '}'])
    review = [ps.stem(word) for word in review if not word in stop_words]
    
    review = ' '.join(review)
    food_paring_corpus_train.append(review)
    
    
food_paring_corpus_test = []

for i in range(len(test_set)):
    review = re.sub('[^a-zA-Z]', ' ', test_set['Food Paring'][i]) 
    review = review.lower() 
    review = review.split() 
    
    ps = PorterStemmer() 
    stop_words = set(stopwords.words('english'))
    # update stop_words
    stop_words.update(['.', ',', '"', "'", ':', ';', '(', ')', '[', ']', '{', '}'])
    review = [ps.stem(word) for word in review if not word in stop_words]
    
    review = ' '.join(review)
    food_paring_corpus_test.append(review)                  

# create the bag of words
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
train_tmp = vectorizer.fit_transform(food_paring_corpus_train)
train_tmp = train_tmp.astype('float16')

test_tmp = vectorizer.transform(food_paring_corpus_test)
test_tmp = test_tmp.astype('float16')

# Note: train_tmp has same number of columns as test_tmp.
# If they were different, there might be problem

from scipy.sparse import hstack
train_set_ohe = hstack((train_set_ohe, train_tmp))
test_set_ohe = hstack((test_set_ohe, test_tmp))
"""


"""
# FIP2:

Food_Paring_dict = dict(train_set["Score"].groupby(train_set["Food Paring"]).mean())
train_set['new_Food_Paring'] = train_set['Food Paring'].map(Food_Paring_dict)
test_set['new_Food_Paring'] = test_set['Food Paring'].map(Food_Paring_dict)

test_set['new_Food_Paring'].fillna(train_set['new_Food_Paring'].mean(), inplace = True)
my_combine(train_set, test_set, 'new_Food_Paring')

"""

# d) 'Glassware Used'

my_combine(train_set, test_set, 'Glassware Used')

"""
# FIP:1

glassware_corpus_train = []

for i in range(len(train_set)):
    review = re.sub('[^a-zA-Z]', ' ', train_set['Glassware Used'][i]) 
    review = review.lower() 
    review = review.split() 
    
    ps = PorterStemmer() 
    stop_words = set(stopwords.words('english'))
    # update stop_words
    stop_words.update(['.', ',', '"', "'", ':', ';', '(', ')', '[', ']', '{', '}'])
    review = [ps.stem(word) for word in review if not word in stop_words]
    
    review = ' '.join(review)
    glassware_corpus_train.append(review)                  
    
    
glassware_corpus_test = []

for i in range(len(test_set)):
    review = re.sub('[^a-zA-Z]', ' ', test_set['Glassware Used'][i]) 
    review = review.lower() 
    review = review.split() 
    
    ps = PorterStemmer() 
    stop_words = set(stopwords.words('english'))
    # update stop_words
    stop_words.update(['.', ',', '"', "'", ':', ';', '(', ')', '[', ']', '{', '}'])
    review = [ps.stem(word) for word in review if not word in stop_words]
    
    review = ' '.join(review)
    glassware_corpus_test.append(review)                  

# create the bag of words
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
train_tmp2 = vectorizer.fit_transform(glassware_corpus_train)
train_tmp2 = train_tmp2.astype('float16')

test_tmp2 = vectorizer.transform(glassware_corpus_test)
test_tmp2 = test_tmp2.astype('float16')

from scipy.sparse import hstack
train_set_ohe = hstack((train_set_ohe, train_tmp2))
test_set_ohe = hstack((test_set_ohe, test_tmp2))
"""

"""
# FIP2:

Glassware_Used_dict = dict(train_set["Score"].groupby(train_set["Glassware Used"]).mean())
train_set['new_Glassware_Used'] = train_set['Glassware Used'].map(Glassware_Used_dict)
test_set['new_Glassware_Used'] = test_set['Glassware Used'].map(Glassware_Used_dict)

my_combine(train_set, test_set, 'new_Glassware_Used')

"""

# e) 'Beer Name' # a unique id given to each beer [categorical variable (not numerical)]
# drop

# FIP:

my_combine(train_set, test_set, 'Beer Name')

# f) 'Ratings'

train_set['Ratings_new'] = train_set['Ratings'].str.replace(",", "")
train_set['Ratings_new'] = train_set['Ratings_new'].astype(int)

test_set['Ratings_new'] = test_set['Ratings'].str.replace(",", "")
test_set['Ratings_new'] = test_set['Ratings_new'].astype(int)

my_combine(train_set, test_set, 'Ratings_new')

"""
train_set_ohe = hstack((train_set_ohe, np.reshape(train_set['Ratings_new'], (len(train_set), 1))))
test_set_ohe = hstack((test_set_ohe, np.reshape(test_set['Ratings_new'], (len(test_set), 1))))
"""

"""
# FIP1: Try categorical variable approach

max(train_set['Ratings_new'])
min(train_set['Ratings_new'])

max(test_set['Ratings_new'])
min(test_set['Ratings_new'])

len(train_set['Ratings_new'].unique()) # 1824
len(test_set['Ratings_new'].unique()) # 600
len(train_test['Ratings_new'].unique()) # 1926
"""

"""
train_set['Ratings_new_logged'] = np.log1p(train_set['Ratings_new'])
test_set['Ratings_new_logged'] = np.log1p(test_set['Ratings_new'])
my_combine(train_set, test_set, 'Ratings_new_logged')
"""

# g) 'Style Name'
my_combine(train_set, test_set, 'Style Name')

"""

FIP1:

Style_Name_dict = dict(train_set["Score"].groupby(train_set["Style Name"]).mean())
train_set['new_Style_Name'] = train_set['Style Name'].map(Style_Name_dict)
test_set['new_Style_Name'] = test_set['Style Name'].map(Style_Name_dict)


# each bin has same frequency (useful when data is skewed)
train_set['new_Style_Name'] = pd.qcut(train_set['new_Style_Name'], 8)
test_set['new_Style_Name'] = pd.qcut(test_set['new_Style_Name'], 8)

# OR

# each bin has equal width 
train_set['new_Style_Name'] = pd.cut(train_set['new_Style_Name'].astype(int), 8)
test_set['new_Style_Name'] = pd.cut(test_set['new_Style_Name'].astype(int), 8)

my_combine(train_set, test_set, 'new_Style_Name')
"""

# h) 'Cellar Temperature'

train_set['Cellar Temperature'].fillna(train_set['Cellar Temperature'].mode()[0], inplace = True)
test_set['Cellar Temperature'].fillna(train_set['Cellar Temperature'].mode()[0], inplace = True)

""" my_combine(train_set, test_set, 'Cellar Temperature') """


# FIP1:

# train_set['Cellar Temperature'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'Cellar Temperature')

"""
shot_area_set = dict(train_set["Score"].groupby(train_set["Cellar Temperature"]).mean())
plt.bar(shot_area_set.keys(), shot_area_set.values())
plt.xlabel('Cellar Temperature')
plt.ylabel('Mean Score')
plt.show()
"""

Cel_Tmp_map = {
'35-40': 1,
'40-45': 2,
'45-50': 3
}

train_set['new_Cellar_Temperature'] = train_set['Cellar Temperature'].map(Cel_Tmp_map)
test_set['new_Cellar_Temperature'] = test_set['Cellar Temperature'].map(Cel_Tmp_map)
my_combine(train_set, test_set, 'new_Cellar_Temperature')

"""
train_set_ohe = hstack((train_set_ohe, np.reshape(train_set['new_Cellar_Temperature'], (len(train_set), 1))))
test_set_ohe = hstack((test_set_ohe, np.reshape(test_set['new_Cellar_Temperature'], (len(test_set), 1))))
"""

# i) 'Serving Temperature'

train_set['Serving Temperature'].fillna(train_set['Serving Temperature'].mode()[0], inplace = True)
test_set['Serving Temperature'].fillna(train_set['Serving Temperature'].mode()[0], inplace = True)

""" my_combine(train_set, test_set, 'Serving Temperature') """


# FIP1:

# train_set['Serving Temperature'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'Serving Temperature')

"""
shot_area_set = dict(train_set["Score"].groupby(train_set["Serving Temperature"]).mean())
plt.bar(shot_area_set.keys(), shot_area_set.values())
plt.xlabel('Serving Temperature')
plt.ylabel('Mean Score')
plt.show()
"""

Ser_Tmp_map = {
'35-40': 1,
'40-45': 2,
'45-50': 3,
'50-55': 4
}

train_set['new_Serving_Temperature'] = train_set['Serving Temperature'].map(Ser_Tmp_map)
test_set['new_Serving_Temperature'] = test_set['Serving Temperature'].map(Ser_Tmp_map)
my_combine(train_set, test_set, 'new_Serving_Temperature')

"""
train_set_ohe = hstack((train_set_ohe, np.reshape(train_set['new_Serving_Temperature'], (len(train_set), 1))))
test_set_ohe = hstack((test_set_ohe, np.reshape(test_set['new_Serving_Temperature'], (len(test_set), 1))))
"""

# Convert to matrices of independent vars

X_train = train_set_ohe.iloc[:, :].values
X_test = test_set_ohe.iloc[:, :].values

"""
X_train = train_set_ohe.toarray()
X_test = test_set_ohe.toarray()
"""

y_train = train_set['Score'] # vector of dependent var

# Feature Scaling

from sklearn.preprocessing import StandardScaler
sc_x = StandardScaler()
X_train = sc_x.fit_transform(X_train)
X_test = sc_x.transform(X_test)

# fitting model 1
!pip install -q catboost

import catboost as cb
train_set_ohe

cat_index = [1, 2, 3, 4, 6]

# FIP1: try dropping some columns
# FIP2: try ANN
# FIP3: common rows in both train and test set
# FIP4: similar kaggle competition


# sum(y_train==0)/len(y_train) # 0.14053856057055747
# sum(y_pred_cb==0)/len(y_pred_cb) # 0.07


reg_cb = cb.CatBoostRegressor(one_hot_max_size=2,
                            depth=9, iterations= 512, l2_leaf_reg= 12, learning_rate= 0.15)

reg_cb.fit(X_train, y_train, cat_features= cat_index)

# predicting results
y_pred_cb = reg_cb.predict(X_test)

"""
max(y_pred_cb)
min(y_pred_cb)
"""

# sum(y_pred_cb < 0)

df = pd.DataFrame()
df['pred'] = y_pred_cb
df.loc[df['pred'] < 0, 'pred'] = 0

y_pred_cb =df['pred']

# prev = 0.42298948

# fitting model 2

# write to file
submission = pd.read_csv("test.csv")
submission['Score'] = y_pred_cb
submission.to_csv('test.csv', index=False)  

files.download('test.csv')